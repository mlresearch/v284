---
title: 'Bridging Neural and Symbolic Computation: A Learnability Study of RNNs on
  Counter and Dyck Languages'
openreview: 0ALxF79u73
abstract: This work presents a neuro-symbolic analysis of the learnability of Recurrent
  Neural Networks (RNNs) in classifying structured formal languages—specifically,
  **counter languages** and **Dyck languages**, which serve as canonical examples
  of context-free and mildly context-sensitive grammars. While prior studies have
  highlighted the expressive power of first-order (LSTM) and second-order (O2RNN)
  architectures within the Chomsky hierarchy, we challenge this perspective by shifting
  the focus from theoretical expressivity to *practical learnability under finite
  precision constraints*. Our results suggest that RNNs function more as finite-state
  machines than stack-based automata when implemented with realistic training regimes
  and embedding representations. We show that classification performance degrades
  sharply as structural similarities between positive and negative sequences increase—highlighting
  a core limitation in the RNN’s ability to internalize hierarchical structure without
  symbolic scaffolding. Interestingly, even simple linear classifiers built on top
  of RNN-derived embeddings outperform chance, underscoring the hidden representational
  capacity within learned states. To probe generalization, we train models on input
  lengths up to 40 and evaluate on lengths extending to 500, using 10 distinct seeds
  to measure statistical robustness. O2RNNs consistently demonstrate greater stability
  and generalization compared to LSTMs, particularly under varied initialization strategies.
  These findings expose the fragility of learned language representations and emphasize
  the role of architectural bias, initialization, and data sampling in determining
  what is truly learnable. Ultimately, our study reframes RNN learnability through
  the lens of *symbolic structure and computational constraints*, advocating for stronger
  formal criteria when assessing neural models’ capacity to reason over structured
  sequences. We argue that expressivity alone is insufficient—**stability, precision,
  and symbolic alignment** are essential for true neuro-symbolic generalization.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dave25a
month: 0
tex_title: 'Bridging Neural and Symbolic Computation: A Learnability Study of RNNs
  on Counter and Dyck Languages'
firstpage: 86
lastpage: 115
page: 86-115
order: 86
cycles: false
bibtex_author: Dave, Neisarg and Kifer, Daniel and Giles, C. Lee and Mali, Ankur
author:
- given: Neisarg
  family: Dave
- given: Daniel
  family: Kifer
- given: C. Lee
  family: Giles
- given: Ankur
  family: Mali
date: 2025-10-07
address:
container-title: Proceedings of The 19th International Conference on Neurosymbolic
  Learning and Reasoning
volume: '284'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v284/main/assets/dave25a/dave25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
