---
title: 'Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond CoT and
  ICL'
section: Poster
openreview: ySTqCi3nqi
abstract: 'Prompt-based reasoning strategies such as Chain-of-Thought (CoT) and In-Context
  Learning (ICL) have become widely used for eliciting reasoning capabilities in Large
  Language Models (LLMs). However, these methods rely on fragile, implicit mechanisms
  often yielding inconsistent outputs across seeds, formats, or minor prompt variations
  making them fundamentally unreliable for tasks requiring stable, interpretable reasoning.
  In contrast, automata-based neuro-symbolic frameworks like RetoMaton offer a more
  structured and trustworthy alternative by grounding retrieval in symbolic memory
  with deterministic transitions. In this work, we extend RetoMaton by replacing its
  global datastore with a local, task-adaptive Weighted Finite Automaton (WFA), constructed
  directly from external domain corpora. This local automaton structure promotes robust,
  context-aware retrieval while preserving symbolic traceability and low inference
  overhead. Unlike prompting, which entangles context and memory in opaque ways, our
  approach leverages the explicit structure of WFAs to provide verifiable and modular
  retrieval behavior, making it better suited for domain transfer and interoperability.
  We evaluate this local RetoMaton variant on two pretrained LLMs LLaMA-3.2-1B and
  Gemma-3-1B-PT across three reasoning tasks: TriviaQA (reading comprehension), GSM8K
  (multi-step math), and MMLU (domain knowledge). Compared to the base model and prompting-based
  methods, augmenting these setups with local RetoMaton consistently improves performance
  while enabling transparent and reproducible retrieval dynamics. Our results highlight
  a promising shift toward trustworthy, symbolic reasoning in modern LLMs via lightweight,
  automaton-guided memory.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mamidala25a
month: 0
tex_title: 'Rethinking Reasoning in LLMs: Neuro-Symbolic Local RetoMaton Beyond CoT
  and ICL'
firstpage: 137
lastpage: 159
page: 137-159
order: 137
cycles: false
bibtex_author: Mamidala, Rushitha Santhoshi and Chhabra, Anshuman and Mali, Ankur
author:
- given: Rushitha Santhoshi
  family: Mamidala
- given: Anshuman
  family: Chhabra
- given: Ankur
  family: Mali
date: 2025-10-07
address:
container-title: Proceedings of The 19th International Conference on Neurosymbolic
  Learning and Reasoning
volume: '284'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v284/main/assets/mamidala25a/mamidala25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
