---
title: 'Grounding Terms from an Ontology for use in Autoformalization: Tokenization
  is All You Need'
section: Poster
software: https://github.com/ontologyportal
openreview: oDygYsPhS8
abstract: Large Language Models (LLMs) have shown strong performance in translating
  natural language into programming languages like Python or Java. However, for niche
  computer languages, where there is limited training data, fine-tuning a base model
  is often necessary. A key challenge arises when the pretrained embeddings of natural
  language terms interfere with the intended syntax and semantics of formal language
  terms. This issue is especially pronounced in the logical language of SUO-KIF, which
  is used in the Suggested Upper Merged Ontology (SUMO). SUMO contains thousands of
  terms that closely resemble everyday English words. As a result, models often produce
  syntactic errors or hallucinate non-existent terms due to conflicting embeddings
  learned during base training. This work introduces a tokenization-based technique
  to mitigate these issues. By altering how formal terms are tokenized, we can decouple
  their embeddings from similar natural language words, significantly reducing syntax
  errors and term hallucinations in the generated formal language output.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: thompson25a
month: 0
tex_title: 'Grounding Terms from an Ontology for use in Autoformalization: Tokenization
  is All You Need'
firstpage: 130
lastpage: 136
page: 130-136
order: 130
cycles: false
bibtex_author: Thompson, Richard and Pease, Adam and K\"{o}lsch, Mathias and Toutsios,
  Angelos
author:
- given: Richard
  family: Thompson
- given: Adam
  family: Pease
- given: Mathias
  family: KÃ¶lsch
- given: Angelos
  family: Toutsios
date: 2025-10-07
address:
container-title: Proceedings of The 19th International Conference on Neurosymbolic
  Learning and Reasoning
volume: '284'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 10
  - 7
pdf: https://raw.githubusercontent.com/mlresearch/v284/main/assets/thompson25a/thompson25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
